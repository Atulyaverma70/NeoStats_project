# ğŸ¤– Customer Support Chatbot (RAG + Multi-Provider)

An AI-powered customer support chatbot built with **Streamlit** and **LangChain**.  
It can:

- Answer questions directly using pre-loaded knowledge (FAQs, manuals, troubleshooting docs).
- Accept uploaded PDF/TXT/JSON files and index them on the fly.
- Choose between **Groq (Llama-3.1)**, **OpenAI GPT-3.5**, or **Google Gemini** models.
- Fall back to Bing web search if no internal answer is found.

---

## ğŸš€ Features

- **Retrieval-Augmented Generation (RAG)**: pulls from `knowledge_docs` or uploaded files.
- **Multi-model Support**: Groq (default), OpenAI, Gemini.
- **Embeddings**: uses GroqEmbeddings if available; falls back to OpenAIEmbeddings.
- **Vector Store**: FAISS for fast similarity search.
- **Web Search**: optional Bing Search integration.
- **Streamlit UI**: chat history, file upload, provider selector.

---

## ğŸ“‚ Project Structure

.
â”œâ”€â”€ app.py # Streamlit main app
â”œâ”€â”€ models/
â”‚ â””â”€â”€ llm.py # Functions to get Groq/OpenAI/Gemini chat models
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ rag_utils.py # Load knowledge_docs & retrieve answers
â”‚ â””â”€â”€ web_search.py # Bing search fallback
â”œâ”€â”€ embeddings.py # Precompute embeddings for local docs
â”œâ”€â”€ config/
â”‚ â””â”€â”€ config.py # API keys here
â”œâ”€â”€ knowledge_docs/ # Your manuals, FAQs, troubleshooting docs
â”‚ â”œâ”€â”€ manuals/
â”‚ â”‚ â””â”€â”€ user_manual.pdf
â”‚ â”œâ”€â”€ faqs/
â”‚ â”‚ â”œâ”€â”€ faqs.txt
â”‚ â”‚ â””â”€â”€ faq.json
â”‚ â””â”€â”€ troubleshooting/
â”‚ â””â”€â”€ troubleshooting.txt
â””â”€â”€ requirements.txt

yaml
Copy code

---

## ğŸ”‘ Setup API Keys

Edit `config/config.py` and set:

```python
OPENAI_API_KEY = "your_openai_key"
GROQ_API_KEY = "your_groq_key"
GEMINI_API_KEY = "your_gemini_key"
BING_API_KEY = "your_bing_key"  # optional, for web_search
ğŸ›  Installation
bash
Copy code
git clone <your-repo>
cd <your-repo>
pip install -r requirements.txt
If you plan to use GroqEmbeddings, install its package:

bash
Copy code
pip install langchain_groq
â–¶ï¸ Running the App
bash
Copy code
streamlit run app.py
Then open the provided localhost URL in your browser.

ğŸ“ Usage
1. Direct Knowledge Base (pre-loaded)
Put your manuals, FAQs, troubleshooting files under knowledge_docs.

The chatbot automatically loads these and can answer directly.

2. Upload Your Own File
In the sidebar, upload a PDF/TXT/JSON file.

The file will be chunked, embedded (Groq/OpenAI), and indexed on-the-fly.

The chatbot will answer using the new knowledge base.

3. Model Selection
Sidebar dropdown lets you choose Groq (default), OpenAI, or Google Gemini.

Responses can be â€œConciseâ€ or â€œDetailed.â€

4. Web Search Fallback
If no internal context found, Bing search snippets are used.

ğŸ“œ Precomputing Embeddings (Optional)
You can precompute all knowledge_docs embeddings and store them in a pickle:

bash
Copy code
python embeddings.py
This creates vector_store.pkl which can be loaded for faster startup.

âš™ï¸ How It Works (Flow)
app.py launches Streamlit UI.

User selects model & uploads file (or uses pre-loaded docs).

process_uploaded_file() â†’ splits & embeds docs â†’ stores in FAISS.

Chat input â†’ builds a context (from FAISS / rag_utils / web_search).

get_chat_model() in models/llm.py returns correct model wrapper.

get_chat_response() sends system+user messages to model â†’ returns answer.

Streamlit displays the conversation.
